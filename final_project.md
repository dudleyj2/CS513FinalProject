# Theory & Practice of Data Cleaning Final Project
#### James Dudley
#### July 31, 2021
#### CS 513

## Introduction
Shared electric scooters first hit consumers in September 2017 when the then start-up, Bird, descended on Santa Monica, California.  Within 14 months, the company was valued at over $2 billion and had a presence in over 100 cities around the globe.  Shortly after that initial launch, more companies like Lyft, Uber, Spin, and Lime introduced their electric scooters.  What began as a novelty quickly drew controversial opinions and government regulations, but has more or less survived and is here to stay as a legitimate form of transportation around city centers.
According to the TomTom Traffic Index Report, traffic across cities worldwide had been steadily increasing for years until the Covid-19 pandemic temporarily reduced the number of commuters and city residents in many metropolitan areas.  After initial shake-out phases and pilot programs, many American cities in particular have embraced scooter programs not only as a way of reducing traffic, but as an extension of public transportation where infrastructure where it is lacking.  Now that the novelty has worn off and people are using scooters as a reasonable way to get from one place to another, there is a lot of potential to gain insight from the data and information generated by the scooters that people are riding.


## Identifying a Dataset
The dataset, <i>D</i>, that has been selected for this project is the [E-Scooter Trips 2020 Pilot Program Dataset](https://data.world/cityofchicago/3rse-fbp6) produced by Data World and the City of Chicago.  For the 2020 Pilot program, the City updated the rules and requirements for e-scooter vendors based on the results of the 2019 data.  

<p align="center">
 <img src=img/chicago_scooter_zones.png/>
    <br>
    <em><b>Image 1:</b> Map of Chicago E-Scooter Operation Zones</em>
</p>


As seen in Image 1 above, the scooters were limited allowed to operate citywide with a handful of exceptions.  Shared electric scooters were prohibited from the Lakefront Trail, Central Business District, and 606 Trail.  Additionally, vendors were limited to 3,333 scooters, had to deploy 50% to the Equity Priority Area shown in Image 1, and scooters could not be ridden on. Sidewalks between 5am and 10pm.  Lastly, riders were required to lock their scooters to a fixed object to end their trip to eliminate the sidewalk clutter and clear pathways for other pedestrians.

This raw dataset consists of ~630,000 rows of scooter commuter data in Chicago, Illinois, from August to December of 2020.  Each row contains 16 columns.  Table 1 below contains each of those columns and their datatypes.



|Column Name	| Description	| Type
|---|---|---|
|Trip ID	|A unique ID for each trip.|Plain Text
|Start Time	|When the trip started, rounded to the nearest hour.|Date & Time
|End Time	|When the trip ended, rounded to the nearest hour.|Date & Time
|Trip Distance	|Trip distance in meters. (Divide by 1609 for miles.)|Number
|Trip Duration	|Trip time in seconds.|Number
|Vendor	|The vendor of the scooter|Plain Text
|Start Community Area Number	|The Community Area number where the trip started.|Number
|End Community Area Number	|The Community Area number where the trip ended.|Number
|Start Community Area Name	|The Community Area name where the trip started.|Plain Text
|End Community Area Name	|The Community Area name where the trip ended.|Plain Text
|Start Centroid Latitude	|The latitude of the center of the trip start Community Area. This column will be blank for locations outside Chicago.|Number
|Start Centroid Longitude	|The longitude of the center of the trip start Community Area. This column will be blank for locations outside Chicago.|Number
|Start Centroid Location	|The location of the center of the trip start Community Area. This column will be blank for locations outside Chicago.|Point
|End Centroid Latitude	|The latitude of the center of the trip end Community Area. This column will be blank for locations outside Chicago.|Number
|End Centroid Longitude	|The longitude of the center of the trip end Community Area. This column will be blank for locations outside Chicago.|Number
|End Centroid Location	|The location of the center of the trip end Community Area. This column will be blank for locations outside Chicago.|Point

<p align="center">
    <em><b>Table 1:</b> Description of Dataset "D" Columns </em>
</p>


## Target Use Cases
With the success of the 2020 Pilot E-Scooter Program, our goal in mind here is to further legitimize the adoption of shared e-scooters as a means of transportation throughout Chicago.  One way that the city can promote the use of alternative methods of transportation outside of automobiles is to increase the number dedicated bike lanes. Thus, our primary target use case, <i>U<sub>1</sub></i>, is to use our data to determine the best placement for future dedicated bike lanes.  In order to get the most accurate locations for our suggested bike lane placements, data cleaning is required.  We want to eliminate certain outliers such as trips with unreasonably long or short durations and unreasonably long or short distances.  Given that our dataset is sufficiently large, we’ll also be removing rows with corrupted or missing data.  More detailed information and the steps for data cleaning are laid out in a later section.

In addition to our primary use case, we have two secondary use cases for our data. <i>U<sub>2</sub></i>, our use case where data cleaning is not necessary, will be to determine which times of the day have the highest earning potential for electric scooters.  The purpose of this is to determine potential surge pricing business models already in place by companies like Lyft and Uber, where the price of the trip is relative to real-time demand. Our other secondary use case, <i>U<sub>3</sub></i>, where data cleaning is not sufficient, will be to determine how many rides it takes for the e-scooters to start turning a profit for their vendors.  


## Raw Dataset Analysis
Before we begin our data cleaning steps, we’re going to need to take a closer look at the data itself.  In order to do so, we’re going to use the [Pandas]( https://pandas.pydata.org/docs/) and [NumPy](https://numpy.org/doc/) Python libraries.

```
import numpy as np
import pandas as pd

df = pd.read_csv("~/Downloads/e-scooter-trips-2020-1.csv")
# Get Total Number of Columns
total_columns = len(df.columns)
print(f"Total Dataset Columns: {total_columns}")

# Get Raw Row Total
raw_row_total = df.size/total_columns

# Remove Rows from Table with NA Values
df.dropna(inplace=True)
na_row_total = df.size/total_columns

print(f"Initial Raw Data Row Count: {raw_row_total}")
print(f"Row Count After Removing NA's: {na_row_total}")
print(f"Total NA Rows Removed: {raw_row_total - na_row_total}")
```
```
Total Dataset Columns: 16
Initial Raw Data Row Count: 630816.0
Row Count After Removing NA's: 629175.0
Total NA Rows Removed: 1641.0
```
Right off the bat, we're removing 1,641 rows from our dataset.  Now that these rows have been removed, let's peek at some important fields of the data to get an idea of important statistical values associated with them, like mean, median, and percentile ranges.

```
print(f"\nTrip Duration Statistics (Seconds)")
print(df["Trip Duration"].describe())

print(f"\nTrip Distance Statistics (Feet)")
print(df["Trip Distance"].describe())
```
```
Trip Duration Statistics (Seconds)
count    629175.000000
mean        993.401151
std        1352.792422
min           0.000000
25%         305.000000
50%         570.000000
75%        1125.000000
max      204182.000000
Name: Trip Duration, dtype: float64

Trip Distance Statistics (Feet)
count    629175.000000
mean       2905.926094
std        3707.496669
min           1.000000
25%         822.000000
50%        1868.000000
75%        3641.000000
max       49997.000000
Name: Trip Distance, dtype: float64
```

As seen in the code output above, we have a wide range of distances and durations, some of which are unreasonable to include in our analysis.  The maximum trip duration, for example, is over 204,000 seconds, or 56 hours and 40 minutes.  This, and an assortment of other entries in our data set are likely due to user error, where the scooter rider forgot to lock the scooter and end their trip.  The minimum duration of 0 seconds isn't long enough to move at all.

Looking at the trip distance statistices, the maximum trip distance, is almost 50,000 feet, or nearly 10 miles. It's also over 13 times the 75th percentile distance of just 3,641 feet, or 0.69 miles. Comparatively, the minimum distance is just 1 foot - much too small to be a legitimate commute.

Needless to say, there are plenty of rows with outlier data that we should remove in order to create <i>D'</i>, our cleaned data set.


## Data Cleaning
The data cleaning process can be broken down into the following steps:
> Step 1: Review for Data Quality Problems
>
> Step 2: Import Raw Data
>
> Step 3: Remove NA Value Columns
>
> Step 4: Remove Unreasonably Long and Short Duration Trips
>
> Step 5: Remove Unreasonably Long and Short Distance Trips
>
> Step 6: Convert Date Datatypes
> 
> Step 7: Export D' for U1

Each of these steps are important to complete in order for us to achieve accurate results for our primary use case, <i>U<sub>1</sub></i>.

#### Step 1: Review for Data Quality Problems
The first step of our data cleaning is to review for obvious data quality problems so that they can be addressed before we begin our work towards our target use case, <i>U<sub>1</sub></i>.  After a visual inspection, it was found that the data is actually fairly clean.  Almost all of the rows include each of the fields listed int he description of the data.  An obvious problem for <i>U<sub>1</sub></i> is that we have not been provided with exact routes for the trips, so we will have to keep that in mind when mapping high scooter traffic areas. Another data quality problem that we cannot work around is that we've been limited to data that was obtained during the Covid-19 pandemic.  This means that there are less work commuters and people in general using the scooters throughout the day, so we have less data to work with.  Additionally, the Start Time and End Time columns are rounded to the nearest hour, so we don't have exact information on when the trips start and end.  With all of this in mind, we can solve some of these issues in the next steps of our data cleaning process.

#### Step 2: Import Raw Data
To do our data cleaning, I've chosen to use the Pandas and NumPy libraries in Python.  The first step, importing our data for cleaning, can be done with a few simple lines of code:

```
import numpy as np
import pandas as pd

df = pd.read_csv("~/Downloads/e-scooter-trips-2020-1.csv")
```

#### Step 3: Remove NA Value Columns
Our second step is also just a few lines of code:
```
# Get Total Number of Columns
total_columns = len(df.columns)
print(f"Total Dataset Columns: {total_columns}")

# Get Raw Row Total
raw_row_total = df.size/total_columns

# Remove Rows from Table with NA Values
df.dropna(inplace=True)
na_row_total = df.size/total_columns

print(f"Initial Raw Data Row Count: {raw_row_total}")
print(f"Row Count After Removing NA's: {na_row_total}")
print(f"Total NA Rows Removed: {raw_row_total - na_row_total}")
```
```
Total Dataset Columns: 16
Initial Raw Data Row Count: 630816.0
Row Count After Removing NA's: 629175.0
Total NA Rows Removed: 1641.0
```
Similar to before, we've removed 1,641 rows for having incomplete data and we're now left with 629,175 rows.

#### Step 4: Remove Unreasonably Long and Short Duration Trips
This step requires a bit more work than the previous two.  For this step, we're going to take a closer look at the trip duration statistics in order to gain a better idea of an acceptable range of values to use.  We begin by finding the mean and standard deviations for Trip Duration in our working data set:
```
# Find Trip Duration Mean
trip_duration_mean = df['Trip Duration'].mean()
print(f'Trip Duration Mean: {trip_duration_mean}')

# Find Trip Duration Standard Deviation
trip_duration_std = df['Trip Duration'].std()
print(f'Trip Duration Standard Deviation: {trip_duration_std}')

# Find +/- 2 SD from Mean
print('Standard Deviation Range = +/-2 St. Dev. from Mean')
print(f'Standard Deviation Range = [{trip_duration_mean - 2*trip_duration_std}, {trip_duration_mean + 2*trip_duration_std}]')
```
```
Trip Duration Mean: 993.4011507132356
Trip Duration Standard Deviation: 1352.7924216882725
Standard Deviation Range = +/-2 St. Dev. from Mean
Standard Deviation Range = [-1712.1836926633096, 3698.9859940897804]
```
As you can see in the output shown above, the distribution of our dataset is not well suited to use a duration range that is within two standard deviation of the mean.  This is due to the nature of the time data - trips can be an absolute minimum of 0 seconds but have a theoretically infinite maximum.  Thus, using a range of values that dips so far below our minimum is not suitable for our use case.

We continue can use the data description to determine a more suitable range for acceptable trip durations:
```
# Describe the Data
print("Data Description")
print(df["Trip Duration"].describe())
print(f"\nMedian Trip Length (Seconds): {df['Trip Duration'].median()}")
print(f"Median Trip Length (Minutes): {df['Trip Duration'].median()/60}")
```
```
Data Description
count    629175.000000
mean        993.401151
std        1352.792422
min           0.000000
25%         305.000000
50%         570.000000
75%        1125.000000
max      204182.000000
Name: Trip Duration, dtype: float64

Median Trip Length (Seconds): 570.0
Median Trip Length (Minutes): 9.5
```

You can see above that our median trip length, also our 50th percentile trip length, is 9.5 minutes.  Using this information, we can try to find reasonable high and low percentile trip durations in order to form our range.
```
print(f"95th Percentile Duration (Seconds): {duration_95th_percentile}")
print(f"95th Percentile Duration (Minutes): {duration_95th_percentile/60}")
```
```
5th Percentile Duration (Seconds): 80.0
5th Percentile Duration (Minutes): 1.3333333333333333
95th Percentile Duration (Seconds): 3390.0
95th Percentile Duration (Minutes): 56.5
```
This 5th and 95th percentile range falls within original standard deviation range and is sufficient for our use case, so to continue with this step, we will remove the rows from the working data set whose durations are outside of the 80 second to 3390 second time range.
```
current_row_total = df.size/total_columns
print(f"Current Working Row Count: {current_row_total}")

df_duration_5th_percentile = df[df["Trip Duration"] < duration_5th_percentile].index
df.drop(df_duration_5th_percentile, inplace=True)
short_trip_row_total = df.size/total_columns
print(f"Rows After Removing Short Durations: {short_trip_row_total}")
print(f"Total Short Duration Rows Removed: {current_row_total - short_trip_row_total}")

df_duration_95th_percentile = df[df["Trip Duration"] > duration_95th_percentile].index
df.drop(df_duration_95th_percentile, inplace=True)
long_trip_row_total = df.size/total_columns
print(f"Rows After Removing Long Durations: {long_trip_row_total}")
print(f"Total Long Duration Rows Removed: {current_row_total - long_trip_row_total}")
```
```
Current Working Row Count: 629175.0
Rows After Removing Short Durations: 598044.0
Total Short Duration Rows Removed: 31131.0
Rows After Removing Long Durations: 566588.0
Total Long Duration Rows Removed: 62587.0
```
After all of that, we've removed another 93,000 rows and we're left with 566,588.


#### Step 5: Remove Unreasonably Long and Short Distance Trips
We continue the work to remove outliers by looking at the data in the Trip Distance columns.  Similarly to before, we explore the mean, standard deviation, and other statistics to help determine the best cutoff points for our acceptable trip distance range.
```
# Find Trip Distance Mean
trip_distance_mean = df["Trip Distance"].mean()
print(f"Trip Distance Mean (Feet): {trip_distance_mean}")

# Find Trip Distance Standard Deviation
trip_distance_std = df["Trip Distance"].std()
print(f"Trip Distance Standard Deviation (Feet): {trip_distance_std}")

# Find +/- 2 SD from Mean
print("Standard Deviation Range (Feet) = +/-2 St. Dev. from Mean")
print(f"Standard Deviation Range (Feet) = [{trip_distance_mean - 2*trip_distance_std}, {trip_distance_mean + 2*trip_distance_std}]")

# Learn here that Mean and Standard Deviation might not be the best approach
# Do a Describe on the Data
print("Data Description")
print(df["Trip Distance"].describe())

# Median here is a much better fit
print(f"Median Trip Distance (Feet): {df['Trip Distance'].median()}")
print(f"Median Trip Distance (Miles): {df['Trip Distance'].median()/5280}")
```
```
Trip Distance Mean (Feet): 2680.593380022168
Trip Distance Standard Deviation (Feet): 2978.9245690711678
Standard Deviation Range (Feet) = +/-2 St. Dev. from Mean
Standard Deviation Range (Feet) = [-3277.2557581201677, 8638.442518164504]
Data Description
count    566588.000000
mean       2680.593380
std        2978.924569
min           1.000000
25%         939.000000
50%        1902.000000
75%        3500.000000
max       49997.000000
Name: Trip Distance, dtype: float64
Median Trip Distance (Feet): 1902.0
Median Trip Distance (Miles): 0.36022727272727273
```
Once again, we find ourselves with an inopportune standard deviation range.  In this case, we can't travel negative distances, and our maximum distance of nearly 50,000 feet, or 9.5 miles, is far greater than the median distance to travel with a scooter 0.36 miles.
As we did before with duration values, we'll find an appropriate percentile range for our data and remove rows that are not in that range.
```
# Get 10th and 90th Percentile Values
distance_10th_percentile = np.percentile(df["Trip Distance"], 10)
distance_90th_percentile = np.percentile(df["Trip Distance"], 90)
print(f"10th Percentile Distance (Feet): {distance_10th_percentile}")
print(f"10th Percentile Distance (Miles): {distance_10th_percentile/5280}")


current_row_total = df.size/total_columns
print(f"Current Working Row Count: {current_row_total}")

df_distance_10th_percentile = df[df["Trip Distance"] < distance_10th_percentile].index
df.drop(df_distance_10th_percentile, inplace=True)
short_trip_row_total = df.size/total_columns
print(f"Rows After Removing Short Distances: {short_trip_row_total}")
print(f"Total Short Distance Rows Removed: {current_row_total - short_trip_row_total}")

df_distance_90th_percentile = df[df["Trip Distance"] > distance_90th_percentile].index
df.drop(df_distance_90th_percentile, inplace=True)
long_trip_row_total = df.size/total_columns
print(f"Rows After Removing Long Distances: {long_trip_row_total}")
print(f"Total Long Distance Rows Removed: {current_row_total - long_trip_row_total}")
```
```
10th Percentile Distance (Feet): 210.0
10th Percentile Distance (Miles): 0.03977272727272727
Current Working Row Count: 566588.0
Rows After Removing Short Distances: 509980.0
Total Short Distance Rows Removed: 56608.0
Rows After Removing Long Distances: 453333.0
Total Long Distance Rows Removed: 113255.0
```
Finally, we're now left with 453,333 rows, all of which have complete data and have data within our acceptable ranges for Trip Duration and Trip Distance.


#### Step 6: Convert Date Datatypes
Our next step is to convert our Date entry data types to something more usable.  The Start Time and End Time values in the working dataset are in plain text/string format.  We need to convert these to a more standardized date format so that they can be used for analysis.  Luckily, this is takes only a few lines of code with our chosen libraries.
```
# Convert Data Types
df['Start Time'].apply(pd.to_datetime)
df['End Time'].apply(pd.to_datetime)
```
Converting these datetimes allows us to use these converted values later on for our use cases.

#### Step 7: Export D' for U1
Now that our dataset is cleaned, we can export it.  This speeds up our work later on, as the data conversion in Step 5 is time consuming and we would like to avoid repeating it.
```
df.to_csv('~/Desktop/output.csv', index=False, header=True)
```


## Target Use Case: U<sub>1</sub>
Once again, our target use case is to determine optimal locations for new bike lanes in Chicago with the intention of promoting reducing vehicle traffic by making alternative means of transportation more available.  The steps to do this are listed below:
> Step 1: Import Cleaned Dataset D' and Required Libraries
>
> Step 2: Discover High Demand Scooter Times
> 
> Step 3: Create Heat Maps of Scooter Trip Locations at High Demand Times
>
> Step 4: Overlay Heat Map with Current Bike Map to Determine New Bike Lane Locations

#### Step 1: Import Cleaned Dataset D' and Required Libraries
As with before, this step is fairly straightforward.  We simply import the clean data set that was exported in our earlier data cleaning process and the new required libraries. 
```
import folium
from folium import plugins
from folium.plugins import HeatMap

import pandas as pd

df = pd.read_csv('~/Desktop/output.csv')
total_columns = len(df.columns)
```
Here, we're importing the [Folium](http://python-visualization.github.io/folium/) library to generate our heatmaps later on.


#### Step 2: Discover High Demand Scooter Times
The first thing that we'll do here is take our dataset D' and partition it twice.  The first partition, dfs_by_hour_start will partition the data based on the start time of the trip, and the second, dfs_by_hour_end, will partition the data based on the end time of the trip.
```
# Separate DF by Start Time Hour
print(f"Calculating Start Times")
hour_starts = [f" {i}:00"[-5:] for i in range(24)]
dfs_by_hour_start = [df[df["Start Time"].str.contains(hour)] for hour in hour_starts]

print(f"Sample of Entries with Start Time at 12AM:")
print(dfs_by_hour_start[0]["Start Time"].head())

print(f"Sample of Entries with STart Time at 4:00PM")
print(dfs_by_hour_start[16]["Start Time"].head())


# Separate DF by End Time Hour
print(f"\n\nCalculating End Times")
hour_ends = [f" {i}:00"[-5:] for i in range(24)]
dfs_by_hour_end = [df[df["End Time"].str.contains(hour)] for hour in hour_ends]

print(f"Sample of Entries with End Time at 12AM:")
print(dfs_by_hour_end[0]["End Time"].head())

print(f"Sample of Entries with End Time at 4:00PM")
print(dfs_by_hour_end[16]["End Time"].head())
```
```
Calculating Start Times
Sample of Entries with Start Time at 12AM:
50794    8/25/20 0:00
50795    8/25/20 0:00
88803    8/31/20 0:00
93988     9/1/20 0:00
93989     9/1/20 0:00
Name: Start Time, dtype: object
Sample of Entries with STart Time at 4:00PM
661    8/12/20 16:00
662    8/12/20 16:00
663    8/12/20 16:00
664    8/12/20 16:00
665    8/12/20 16:00
Name: Start Time, dtype: object


Calculating End Times
Sample of Entries with End Time at 12AM:
50794    8/25/20 0:00
50795    8/25/20 0:00
88803    8/31/20 0:00
93987     9/1/20 0:00
93988     9/1/20 0:00
Name: End Time, dtype: object
Sample of Entries with End Time at 4:00PM
555    8/12/20 16:00
581    8/12/20 16:00
584    8/12/20 16:00
591    8/12/20 16:00
598    8/12/20 16:00
Name: End Time, dtype: object
```

Now, we'll take these partitioned dataframes to determine how many entries there are per hour.
```
# Get Total Rows/Hour
trips_per_hour_start = [hour.size/total_columns for hour in dfs_by_hour_start]

print(f'Calculating End Times')
dfs_by_hour_end = [df[df['End Time'].str.contains(hour)] for hour in hour_starts]

for hour in dfs_by_hour_end:
    print(hour['End Time'].head())

# Get Total Rows/Hour
trips_per_hour_end = [hour.size/total_columns for hour in dfs_by_hour_end]

print(f'Trips per Hour Start: ')
for i in range(len(trips_per_hour_start)):
    print(f'Hour {i}: {trips_per_hour_start[i]}')

print(f'\n\nTrips per Hour End: ')
for i in range(len(trips_per_hour_end)):
    print(f'Hour {i}: {trips_per_hour_end[i]}')
```
```
Trips per Hour Start: 
Hour 0: 14.0
Hour 1: 8.0
Hour 2: 1.0
Hour 3: 18.0
Hour 4: 9.0
Hour 5: 2420.0
Hour 6: 4851.0
Hour 7: 7977.0
Hour 8: 11478.0
Hour 9: 13552.0
Hour 10: 17965.0
Hour 11: 24095.0
Hour 12: 29691.0
Hour 13: 31807.0
Hour 14: 34327.0
Hour 15: 39004.0
Hour 16: 42860.0
Hour 17: 47896.0
Hour 18: 47740.0
Hour 19: 40485.0
Hour 20: 31848.0
Hour 21: 25075.0
Hour 22: 185.0
Hour 23: 27.0


Trips per Hour End: 
Hour 0: 17.0
Hour 1: 7.0
Hour 2: 2.0
Hour 3: 18.0
Hour 4: 7.0
Hour 5: 2032.0
Hour 6: 4369.0
Hour 7: 7576.0
Hour 8: 10861.0
Hour 9: 12914.0
Hour 10: 17024.0
Hour 11: 22649.0
Hour 12: 28858.0
Hour 13: 31328.0
Hour 14: 33446.0
Hour 15: 38092.0
Hour 16: 41945.0
Hour 17: 47139.0
Hour 18: 48060.0
Hour 19: 42213.0
Hour 20: 33211.0
Hour 21: 26438.0
Hour 22: 5096.0
Hour 23: 31.0
```

Copying and pasting these values into excel, we're able to create Figure 1, which helps to visualize scooter demand each hour.

<p align="center">
 <img src=img/chart.png/>
    <br>
    <em><b>Figure 1:</b> Scooter Trips by Hour Start and End</em>
</p>

As you can see in Figure 1, the start and end time lines are very similar.  It makes sense that the orange "End Time" chart line is shifted the slightest bit to the right, because the end times (with rounding) must be equal to or greater than the start times. The shift is very small because our median time for this data set is only a few minutes.  Altogether, this chart shows us that the peak times for commuting on scooters are between 16:00 and 19:00, or 4:00PM and 7:00PM. Because the peaks are so similar, we'll use the Start Time values for creating our heat maps in Step 3.
We can also see in this chart that the early and late hours in the day have very few entries in comparison to the peak times.  This is because the scooters are charged every night and less people are out.


#### Step 3: Create Heat Maps of Scooter Trip Locations at High Demand Times
Our next step is to make heatmaps for the high demand times that the scooters are in use.  We'll do this using the folium library, which was imported in Step 1.  These heatmaps will be exported as html files.

```
# Make Heat Maps
df_16 = df[df['Start Time'].str.contains('16:00')]
df_17 = df[df['Start Time'].str.contains('17:00')]
df_18 = df[df['Start Time'].str.contains('18:00')]
df_19 = df[df['Start Time'].str.contains('19:00')]


my_heatmap_16 = folium.Map([41.8781, -87.6298], zoom_start=11)
heat_data_start_16 = [[row['Start Centroid Latitude'],row['Start Centroid Longitude']] for index, row in df_16.iterrows()]
HeatMap(heat_data_start_16, blur=30, radiues=10).add_to(my_heatmap_16)
my_heatmap_16.save("1600.html")

my_heatmap_17 = folium.Map([41.8781, -87.6298], zoom_start=11)
heat_data_start_17 = [[row['Start Centroid Latitude'],row['Start Centroid Longitude']] for index, row in df_17.iterrows()]
HeatMap(heat_data_start_17, blur=30, radiues=10).add_to(my_heatmap_17)
my_heatmap_17.save("1700.html")

my_heatmap_18 = folium.Map([41.8781, -87.6298], zoom_start=11)
heat_data_start_18 = [[row['Start Centroid Latitude'],row['Start Centroid Longitude']] for index, row in df_18.iterrows()]
HeatMap(heat_data_start_18, blur=30, radiues=10).add_to(my_heatmap_18)
my_heatmap_18.save("1800.html")

my_heatmap_19 = folium.Map([41.8781, -87.6298], zoom_start=11)
heat_data_start_19 = [[row['Start Centroid Latitude'],row['Start Centroid Longitude']] for index, row in df_19.iterrows()]
HeatMap(heat_data_start_19, blur=30, radiues=10).add_to(my_heatmap_19)
my_heatmap_19.save("1900.jpg")
```
The resulting heatmaps are nearly identical and shown below.

<p align="center">
 <img src=img/1600.png/>
    <br>
    <em><b>Image 2:</b> Heat Map of Scooters with Start Time == 16:00 Hours</em>
</p>

<p align="center">
 <img src=img/1600.png/>
    <br>
    <em><b>Image 3:</b> Heat Map of Scooters with Start Time == 17:00 Hours</em>
</p>

<p align="center">
 <img src=img/1600.png/>
    <br>
    <em><b>Image 4:</b> Heat Map of Scooters with Start Time == 18:00 Hours</em>
</p>

<p align="center">
 <img src=img/1600.png/>
    <br>
    <em><b>Image 5:</b> Heat Map of Scooters with Start Time == 19:00 Hours</em>
</p>